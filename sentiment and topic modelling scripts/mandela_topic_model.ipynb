{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "#topic modelling performed on both normal vader sentiment scores and scores without neutral\n",
    "filenames = [\n",
    "            \"outputs/mandela_vader_neutral_speech_sentiment_analysis_scores.csv\", \n",
    "            \"outputs/mandela_vader_pos_neg_speech_sentiment_analysis_scores.csv\",\n",
    "            \"outputs/mandela_vader_strong_non_compound_scores_sentiment_low.csv\",\n",
    "            \"outputs/mandela_vader_strong_non_compound_scores_sentiment_high.csv\"\n",
    "            ]            \n",
    "\n",
    "#create dataframe from sentiment analysis csv data\n",
    "sentiment_dfs = []\n",
    "for file in filenames:\n",
    "    df = pd.read_csv(file)\n",
    "    sentiment_dfs.append(df)\n",
    "\n",
    "#clean text to make consistent and remove punctuation to help with removing duplicates\n",
    "#duplicates can have slightly different punctuation\n",
    "#remove duplicate sentences from different sources to improve topic modelling performance\n",
    "for df in sentiment_dfs:\n",
    "    df[\"no_punct_sentence\"] = df[\"sentence\"].str.replace(\"[^\\w\\s]\", \"\", regex=True).replace('\\n', ' ').replace(u'\\xa0', u' ').replace('& ', 'and ').replace('â€”', ' ')\n",
    "    df.drop_duplicates(\"no_punct_sentence\", inplace = True)    \n",
    "\n",
    "# basename returns filename removing directory path.\n",
    "# split to remove \".xml\" extension so that we can use it for charts etc further down\n",
    "file_bases = []\n",
    "for file in filenames:\n",
    "    filename = os.path.basename(file)\n",
    "    filename = filename.split(\".\")[0]\n",
    "    file_bases.append(filename)\n",
    "\n",
    "# Create directories if they don't exist\n",
    "output_dir = \"outputs/top_mod/\"\n",
    "df_heads_dir = \"outputs/top_mod/df_heads/\"\n",
    "images_dir = \"outputs/top_mod/images/\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(df_heads_dir, exist_ok=True)\n",
    "os.makedirs(images_dir, exist_ok=True)\n",
    "\n",
    "#loop through dataframes for each sentiment analysis category\n",
    "#file bases looped through for use in visualisations\n",
    "for sent_df, file_base in zip(sentiment_dfs, file_bases):\n",
    "    \n",
    "    #create lists for all sentences in file and their years\n",
    "    sentences = list(sent_df[\"no_punct_sentence\"])\n",
    "    years = list(sent_df[\"year\"])\n",
    "\n",
    "    #create stopwords and extend to remove common words specific to corpus that will not aid analysis\n",
    "    #this list can be extended following initial results and then run code again for better results\n",
    "    nltk_stopwords = stopwords.words(\"english\")\n",
    "    nltk_stopwords.extend([\n",
    "                            \"hon\", \"honourable\", \"would\", \"hear\", \"whether\", \"member\", \"cheers\", \"moved\", \"applause\",\n",
    "                            \"mr\", \"said\", \"gentleman\", \"ladies\", \"debate\", \"adjournment\", \"house\", \"member\", \"mp\",\n",
    "                            \"objection\", \"members\", \"could\", \"never\", \"us\", \"may\", \"upon\", \"better\", \"shall\",\n",
    "                            \"confident\", \"brought\", \"say\", \"way\", \"important\", \"well\", \"great\", \"see\", \"think\",\n",
    "                            \"show\", \"however\", \"also\", \"little\", \"received\", \"ever\", \"many\", \"might\", \"means\",\n",
    "                            \"called\", \"held\", \"view\", \"considered\", \"course\", \"called\", \"part\", \"one\", \"must\",\n",
    "                            \"day\", \"part\", \"present\", \"put\", \"done\", \"sir\", \"believe\", \"name\", \"three\", \"without\",\n",
    "                            \"true\", \"give\", \"came\", \"come\", \"thought\", \"certainly\", \"last\", \"made\", \"best\", \"wish\",\n",
    "                            \"like\", \"done\", \"kind\", \"know\", \"regard\", \"much\", \"know\", \"ask\", \"last\", \"consider\",\n",
    "                            \"made\", \"must\", \"told\", \"since\", \"let\", \"led\", \"distinguished\", \"excellencies\", \"excellency\",\n",
    "                            \"delegates\", \"guests\", \"observers\", \"highness\", \"lord\", \"highnesses\", \"majesty\", \"majesties\",\n",
    "                            \"madam\", \"delegation\", \"delegations\", \"minister\", \"chairperson\", \"chairman\", \"esteemed\", \n",
    "                            \"premier\", \"royal\", \"master\", \"comrade\"\n",
    "                            ])\n",
    "\n",
    "    #tokenize text into individual words, omitting any stopwords as well\n",
    "    non_stop_sents = []\n",
    "    for sentence in sentences:\n",
    "        non_stop_tokens = [i for i in sentence.lower().split() if i not in nltk_stopwords]\n",
    "        non_stop_sent = \" \".join(non_stop_tokens)\n",
    "        non_stop_sents.append(non_stop_sent)\n",
    "        \n",
    "    #create vectorizer to produce tfidf scores for contents of each file\n",
    "    #max_df - maximum percentage of sentences a word can appear in - avoid too frequent words\n",
    "    #min_df - minimum number of sentences with word - avoid too infrequent words\n",
    "    #max_features - limit analysis to top words by frequency across all sentences\n",
    "    #lowercase establishes word consistency across sentences      \n",
    "    vectorizer = TfidfVectorizer(\n",
    "                                    lowercase=True,\n",
    "                                    max_df = 0.9,\n",
    "                                    min_df = 3,\n",
    "                                    ngram_range = (1,2),\n",
    "                                    max_features = 200\n",
    "                                )\n",
    "\n",
    "    #apply vectorizer to list of file contents        \n",
    "    transformed_docs = vectorizer.fit_transform(non_stop_sents)\n",
    "\n",
    "    #tfidf results as numeric arrays\n",
    "    transformed_docs_as_array = transformed_docs.toarray()\n",
    "\n",
    "    #for each array get feature names and tfidf scores for words for each sentence\n",
    "    #convert word and score into a list of tuples for each sentence\n",
    "    #convert list of tuples into a dataframe for each sentence\n",
    "    #remove words with score of zero\n",
    "    dfs = []\n",
    "    for doc in transformed_docs_as_array:\n",
    "        tf_idf_tuples = list(zip(vectorizer.get_feature_names_out(), doc))       \n",
    "        doc_as_df = pd.DataFrame.from_records(tf_idf_tuples, columns=[\"term\", \"score\"]).sort_values(by=\"score\", ascending=False).reset_index(drop=True)\n",
    "        doc_as_df.drop(doc_as_df.index[20:], inplace=True)\n",
    "        doc_non_zero = doc_as_df.loc[(doc_as_df['score'] != 0)]\n",
    "        dfs.append(doc_non_zero)\n",
    "    \n",
    "    #add publication year of source for each sentence\n",
    "    #sentences will ultimately be divided by year to find patterns by timeframe\n",
    "    dfs_2 = []\n",
    "    for year, df in zip(years, dfs):\n",
    "        df[\"year\"] = year\n",
    "        dfs_2.append(df)\n",
    "               \n",
    "    #combine results for each sentence into a single dataframe, maintain row as sentence results\n",
    "    #group sentences by year\n",
    "    dfs_3 = [df.set_index(\"year\") for df in dfs_2]\n",
    "    df_all = pd.concat(dfs_3)\n",
    "    df_grouped = df_all.groupby(\"year\")\n",
    "    \n",
    "    #loop for csv and visualisation outputs for each year group of sentences\n",
    "    for name, group in df_grouped:\n",
    "\n",
    "        #save initial results to csv\n",
    "        group.to_csv(f\"outputs/top_mod/{name}_{file_base}_topic_model.csv\")\n",
    "\n",
    "        #create new csv which combines the scores for terms with high tfidf scores across multiple sentences\n",
    "        combined_df = group.groupby(\"term\").sum().sort_values(by=\"score\", ascending=False)\n",
    "        combined_df.to_csv(f\"outputs/top_mod/{name}_{file_base}_combined_score_topic_model.csv\")\n",
    "                \n",
    "        #set font for chart\n",
    "        font = {\"fontname\":\"Arial\"}\n",
    "        \n",
    "        #limit final combined term results across all sentences for purpose of good visualisation, number can be amended\n",
    "        comb_df_head = combined_df.head(n=50)\n",
    "        \n",
    "        #create axis for bar chart\n",
    "        ax = comb_df_head.plot.bar(legend=None)\n",
    "        \n",
    "        #remove outline on the right and top of chart\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        \n",
    "        #set titles and font for all text in chart\n",
    "        plt.title(f\"{name} {file_base} \\n Highest Scoring Sentence Topics\", **font, fontsize=10)\n",
    "        plt.xlabel(\"Terms\", **font)\n",
    "        plt.ylabel(\"Combined TFIDF Score\", **font)\n",
    "        plt.xticks(**font)\n",
    "        plt.yticks(**font)\n",
    "        \n",
    "        #set tight layout to ensure all text visible and save/show plot\n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(0.2)\n",
    "        plt.savefig(f\"outputs/top_mod/images/{name}_{file_base}_highest_scoring_topics.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        #output top 50 to csv\n",
    "        comb_df_head.insert(1, \"date\", name)\n",
    "        comb_df_head.to_csv(f\"outputs/top_mod/df_heads/{name}_{file_base}_combined_score_df_head.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
