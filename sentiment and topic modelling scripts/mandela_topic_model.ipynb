{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 94\u001b[0m\n\u001b[0;32m     85\u001b[0m vectorizer \u001b[39m=\u001b[39m TfidfVectorizer(\n\u001b[0;32m     86\u001b[0m                                 lowercase\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     87\u001b[0m                                 max_df \u001b[39m=\u001b[39m \u001b[39m0.9\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     90\u001b[0m                                 max_features \u001b[39m=\u001b[39m \u001b[39m200\u001b[39m\n\u001b[0;32m     91\u001b[0m                             )\n\u001b[0;32m     93\u001b[0m \u001b[39m#apply vectorizer to list of file contents        \u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m transformed_docs \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39;49mfit_transform(non_stop_sents)\n\u001b[0;32m     96\u001b[0m \u001b[39m#tfidf results as numeric arrays\u001b[39;00m\n\u001b[0;32m     97\u001b[0m transformed_docs_as_array \u001b[39m=\u001b[39m transformed_docs\u001b[39m.\u001b[39mtoarray()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:2133\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2126\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params()\n\u001b[0;32m   2127\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2128\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[0;32m   2129\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[0;32m   2130\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[0;32m   2131\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[0;32m   2132\u001b[0m )\n\u001b[1;32m-> 2133\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[0;32m   2134\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[0;32m   2135\u001b[0m \u001b[39m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2136\u001b[0m \u001b[39m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:1388\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1380\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1381\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m             )\n\u001b[0;32m   1386\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1390\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1391\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:1294\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1292\u001b[0m     vocabulary \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1293\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1294\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1295\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1296\u001b[0m         )\n\u001b[0;32m   1298\u001b[0m \u001b[39mif\u001b[39;00m indptr[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m np\u001b[39m.\u001b[39miinfo(np\u001b[39m.\u001b[39mint32)\u001b[39m.\u001b[39mmax:  \u001b[39m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1299\u001b[0m     \u001b[39mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "#topic modelling performed on both normal vader sentiment scores and scores without neutral\n",
    "filenames = [\n",
    "            \"outputs/mandela_vader_neutral_speech_sentiment_analysis_scores.csv\", \n",
    "            \"outputs/mandela_vader_pos_neg_speech_sentiment_analysis_scores.csv\",\n",
    "            \"outputs/mandela_vader_strong_non_compound_scores_sentiment_low.csv\",\n",
    "            \"outputs/mandela_vader_strong_non_compound_scores_sentiment_high.csv\"\n",
    "            ]            \n",
    "\n",
    "#create dataframe from sentiment analysis csv data\n",
    "sentiment_dfs = []\n",
    "for file in filenames:\n",
    "    df = pd.read_csv(file)\n",
    "    sentiment_dfs.append(df)\n",
    "\n",
    "#clean text to make consistent and remove punctuation to help with removing duplicates\n",
    "#duplicates can have slightly different punctuation\n",
    "#remove duplicate sentences from different sources to improve topic modelling performance\n",
    "for df in sentiment_dfs:\n",
    "    df[\"no_punct_sentence\"] = df[\"sentence\"].str.replace(\"[^\\w\\s]\", \"\", regex=True).replace('\\n', ' ').replace(u'\\xa0', u' ').replace('& ', 'and ').replace('â€”', ' ')\n",
    "    df.drop_duplicates(\"no_punct_sentence\", inplace = True)    \n",
    "\n",
    "# basename returns filename removing directory path.\n",
    "# split to remove \".xml\" extension so that we can use it for charts etc further down\n",
    "file_bases = []\n",
    "for file in filenames:\n",
    "    filename = os.path.basename(file)\n",
    "    filename = filename.split(\".\")[0]\n",
    "    file_bases.append(filename)\n",
    "\n",
    "# Create directories if they don't exist\n",
    "output_dir = \"outputs/top_mod/\"\n",
    "df_heads_dir = \"outputs/top_mod/df_heads/\"\n",
    "images_dir = \"outputs/top_mod/images/\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(df_heads_dir, exist_ok=True)\n",
    "os.makedirs(images_dir, exist_ok=True)\n",
    "\n",
    "#loop through dataframes for each sentiment analysis category\n",
    "#file bases looped through for use in visualisations\n",
    "for sent_df, file_base in zip(sentiment_dfs, file_bases):\n",
    "    \n",
    "    #create lists for all sentences in file and their years\n",
    "    sentences = list(sent_df[\"no_punct_sentence\"])\n",
    "    years = list(sent_df[\"year\"])\n",
    "\n",
    "    #create stopwords and extend to remove common words specific to corpus that will not aid analysis\n",
    "    #this list can be extended following initial results and then run code again for better results\n",
    "    nltk_stopwords = stopwords.words(\"english\")\n",
    "    nltk_stopwords.extend([\n",
    "                            \"hon\", \"would\", \"hear\", \"whether\", \"member\", \"cheers\", \"moved\", \"applause\",\n",
    "                            \"mr\", \"said\", \"gentleman\", \"debate\", \"adjournment\", \"house\", \"member\", \"mp\",\n",
    "                            \"objection\", \"members\", \"could\", \"never\", \"us\", \"may\", \"upon\", \"better\", \"shall\",\n",
    "                            \"confident\", \"brought\", \"say\", \"way\", \"important\", \"well\", \"great\", \"see\", \"think\",\n",
    "                            \"show\", \"however\", \"also\", \"little\", \"received\", \"ever\", \"many\", \"might\", \"means\",\n",
    "                            \"called\", \"held\", \"view\", \"considered\", \"course\", \"called\", \"part\", \"one\", \"must\",\n",
    "                            \"day\", \"part\", \"present\", \"put\", \"done\", \"sir\", \"believe\", \"name\", \"three\", \"without\",\n",
    "                            \"true\", \"give\", \"came\", \"come\", \"thought\", \"certainly\", \"last\", \"made\", \"best\", \"wish\",\n",
    "                            \"like\", \"done\", \"kind\", \"know\", \"regard\", \"much\", \"know\", \"ask\", \"last\", \"consider\",\n",
    "                            \"made\", \"must\", \"told\", \"since\", \"let\", \"led\"\n",
    "                            ])\n",
    "\n",
    "    #tokenize text into individual words, omitting any stopwords as well\n",
    "    non_stop_sents = []\n",
    "    for sentence in sentences:\n",
    "        non_stop_tokens = [i for i in sentence.lower().split() if i not in nltk_stopwords]\n",
    "        non_stop_sent = \" \".join(non_stop_tokens)\n",
    "        non_stop_sents.append(non_stop_sent)\n",
    "        \n",
    "    #create vectorizer to produce tfidf scores for contents of each file\n",
    "    #max_df - maximum percentage of sentences a word can appear in - avoid too frequent words\n",
    "    #min_df - minimum number of sentences with word - avoid too infrequent words\n",
    "    #max_features - limit analysis to top words by frequency across all sentences\n",
    "    #lowercase establishes word consistency across sentences      \n",
    "    vectorizer = TfidfVectorizer(\n",
    "                                    lowercase=True,\n",
    "                                    max_df = 0.9,\n",
    "                                    min_df = 3,\n",
    "                                    ngram_range = (1,2),\n",
    "                                    max_features = 200\n",
    "                                )\n",
    "\n",
    "    #apply vectorizer to list of file contents        \n",
    "    transformed_docs = vectorizer.fit_transform(non_stop_sents)\n",
    "\n",
    "    #tfidf results as numeric arrays\n",
    "    transformed_docs_as_array = transformed_docs.toarray()\n",
    "\n",
    "    #for each array get feature names and tfidf scores for words for each sentence\n",
    "    #convert word and score into a list of tuples for each sentence\n",
    "    #convert list of tuples into a dataframe for each sentence\n",
    "    #remove words with score of zero\n",
    "    dfs = []\n",
    "    for doc in transformed_docs_as_array:\n",
    "        tf_idf_tuples = list(zip(vectorizer.get_feature_names_out(), doc))       \n",
    "        doc_as_df = pd.DataFrame.from_records(tf_idf_tuples, columns=[\"term\", \"score\"]).sort_values(by=\"score\", ascending=False).reset_index(drop=True)\n",
    "        doc_as_df.drop(doc_as_df.index[20:], inplace=True)\n",
    "        doc_non_zero = doc_as_df.loc[(doc_as_df['score'] != 0)]\n",
    "        dfs.append(doc_non_zero)\n",
    "    \n",
    "    #add publication year of source for each sentence\n",
    "    #sentences will ultimately be divided by year to find patterns by timeframe\n",
    "    dfs_2 = []\n",
    "    for year, df in zip(years, dfs):\n",
    "        df[\"year\"] = year\n",
    "        dfs_2.append(df)\n",
    "               \n",
    "    #combine results for each sentence into a single dataframe, maintain row as sentence results\n",
    "    #group sentences by year\n",
    "    dfs_3 = [df.set_index(\"year\") for df in dfs_2]\n",
    "    df_all = pd.concat(dfs_3)\n",
    "    df_grouped = df_all.groupby(\"year\")\n",
    "    \n",
    "    #loop for csv and visualisation outputs for each year group of sentences\n",
    "    for name, group in df_grouped:\n",
    "\n",
    "        #save initial results to csv\n",
    "        group.to_csv(f\"outputs/top_mod/{name}_{file_base}_topic_model.csv\")\n",
    "\n",
    "        #create new csv which combines the scores for terms with high tfidf scores across multiple sentences\n",
    "        combined_df = group.groupby(\"term\").sum().sort_values(by=\"score\", ascending=False)\n",
    "        combined_df.to_csv(f\"outputs/top_mod/{name}_{file_base}_combined_score_topic_model.csv\")\n",
    "                \n",
    "        #set font for chart\n",
    "        font = {\"fontname\":\"Arial\"}\n",
    "        \n",
    "        #limit final combined term results across all sentences for purpose of good visualisation, number can be amended\n",
    "        comb_df_head = combined_df.head(n=50)\n",
    "        \n",
    "        #create axis for bar chart\n",
    "        ax = comb_df_head.plot.bar(legend=None)\n",
    "        \n",
    "        #remove outline on the right and top of chart\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        \n",
    "        #set titles and font for all text in chart\n",
    "        plt.title(f\"{name} {file_base} \\n Highest Scoring Sentence Topics\", **font, fontsize=10)\n",
    "        plt.xlabel(\"Terms\", **font)\n",
    "        plt.ylabel(\"Combined TFIDF Score\", **font)\n",
    "        plt.xticks(**font)\n",
    "        plt.yticks(**font)\n",
    "        \n",
    "        #set tight layout to ensure all text visible and save/show plot\n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(0.2)\n",
    "        plt.savefig(f\"outputs/top_mod/images/{name}_{file_base}_highest_scoring_topics.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        #output top 50 to csv\n",
    "        comb_df_head.insert(1, \"date\", name)\n",
    "        comb_df_head.to_csv(f\"outputs/top_mod/df_heads/{name}_{file_base}_combined_score_df_head.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
