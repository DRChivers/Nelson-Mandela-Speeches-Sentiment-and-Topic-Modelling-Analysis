{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 12-13: malformed \\N character escape (2182886489.py, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 19\u001b[1;36m\u001b[0m\n\u001b[1;33m    dir_path = Path(\"C:\\Documents\\Nelson-Mandela-Speeches-Sentiment-and-Topic-Modelling-Analysis\\speeches\\new xml\")\u001b[0m\n\u001b[1;37m                                                                                                                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 12-13: malformed \\N character escape\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import nltk \n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from pathlib import Path\n",
    "from natsort import os_sorted\n",
    "\n",
    "# Set NLTK data path and download vader_lexicon directly\n",
    "nltk.data.path.append(Path(r\"C:/Users/Dominic's PC/nltk_data\").expanduser())\n",
    "nltk.download(\"vader_lexicon\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "#create a filepath to the directory\n",
    "dir_path = Path(\"C:/Documents/Nelson-Mandela-Speeches-Sentiment-and-Topic-Modelling-Analysis/speeches/new xml\") \n",
    "\n",
    "#get filepaths to all the xml files in the directory\n",
    "xml_files = (file for file in dir_path.iterdir() if file.is_file() and file.name.lower().endswith('.xml'))\n",
    "\n",
    "#sort files numerically as in a system file directory\n",
    "xml_files = os_sorted(xml_files)\n",
    "\n",
    "#basename returns filename removing directory path.\n",
    "#split to remove \".xml\" extension so that we can use later in dataframe\n",
    "filenames = []\n",
    "for path in xml_files:\n",
    "    filename = os.path.basename(path)\n",
    "    filename = filename.split(\".\")[0]\n",
    "    filenames.append(filename)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = Path(\"outputs\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "#open xml files and convert into beautiful soup object for text extraction from xml elements\n",
    "all_docs = []\n",
    "for file in xml_files:\n",
    "    with file.open('r', encoding='utf-8') as xml:\n",
    "        doc = BeautifulSoup(xml, \"lxml-xml\")\n",
    "        all_docs.append(doc)\n",
    "\n",
    "#load nltk sentence detector for dividing text into sentences\n",
    "#load full stop abbreviations to sentence detector to prevent them from ending sentences \n",
    "punkt_param = PunktParameters()\n",
    "punkt_param.abbrev_types = set([\"hon\", \"mr\", \"rev\", \"dr\", \"m.p\", \"c.s\", \"c.v\", \"c.e\", \"t.l\", \"j.r\", \"j.j\", \"a.j\",\n",
    "                                \"r.b\", \"j.g\", \"j.l\", \"j.r\" \"patk\", \"j.f\", \"n.b\", \"p.j\", \"c.j\", \"t.d\", \"r\", \"p.p\",\n",
    "                                \"c.c\", \"wm\", \"capt\"])\n",
    "sentence_tokenizer = PunktSentenceTokenizer(punkt_param)   \n",
    "\n",
    "#new words added to sentiment analyzer with polarity score, load dictionary from external JSON file\n",
    "with open(\"C:/Documents/Nelson-Mandela-Speeches-Sentiment-and-Topic-Modelling-Analysis/sentiment and topic modelling scripts/sentiment_analyser_edit_lexicon.json\") as data:\n",
    "    sent_analyser_words = json.load(data)\n",
    "\n",
    "#initialize sentiment analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "#update sentiment analyzer lexicon with new words from JSON file opened above\n",
    "sid.lexicon.update(sent_analyser_words)\n",
    "\n",
    "#lists for dataframes for each sentence in corpus\n",
    "files = []\n",
    "speech_ids = []\n",
    "years = []\n",
    "sentence_list = []\n",
    "scores = []\n",
    "\n",
    "#extract filename for each file\n",
    "#extract text from each file with corresponding filename\n",
    "#extract speech id that file refers to\n",
    "#extract year of source publication\n",
    "file_doc = zip(filenames, all_docs)\n",
    "for file, doc in file_doc:      \n",
    "    speech = doc.find(\"term\", {\"key\": True})\n",
    "    if speech is not None:\n",
    "        speech = speech[\"key\"]\n",
    "    else:\n",
    "        speech = \"\"\n",
    "    date = doc.find(\"date\")\n",
    "    if date is not None and \"when\" in date.attrs:\n",
    "        year_text = date[\"when\"]\n",
    "        year = re.match(r\"\\d{4}\", year_text)\n",
    "        if year != None:\n",
    "            year = year.group()\n",
    "        else:\n",
    "            year = \"\"\n",
    "    else:\n",
    "        year = \"\"\n",
    "    \n",
    "    #extract body text from body element, remove bracketed text\n",
    "    #clean text to ensure words are separate and divide text into sentences\n",
    "    #perform sentiment analysis on sentence using model initialised above\n",
    "    #append filename, speech_id, publication year, sentence and score to above lists for each sentence  \n",
    "    body = doc.find_all(\"body\")\n",
    "    for text in body:\n",
    "        text = text.get_text()\n",
    "        text_non_bracket = re.sub(r\"\\(.*?\\)|\\[.*?\\]\", \"\", text)\n",
    "        text_clean = text_non_bracket.strip().replace(\"\\n\", \" \").replace(\"-\", \" \").replace(\"â€”\", \" \")\n",
    "        sentences = sentence_tokenizer.tokenize(text_clean)\n",
    "        for sentence in sentences:\n",
    "            files.append(file)\n",
    "            speech_ids.append(speech)\n",
    "            years.append(year)\n",
    "            sentence_list.append(sentence)\n",
    "            score = sid.polarity_scores(sentence)\n",
    "            scores.append(score)\n",
    "\n",
    "#creation of word lists so we can see how model judges polarity in relation to our corpus  \n",
    "#split sentence words for the corpus into separate tokens\n",
    "#use sid.polarity scores to find the individual sentiment score of each word as given by the sentiment model\n",
    "#append to positive, negative or neutral word list accordingly\n",
    "#word scores run from -4 to 4\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for sentence in sentence_list:\n",
    "    nltk.download('punkt')\n",
    "    tokenized_sentence = word_tokenize(sentence)\n",
    "    for word in tokenized_sentence:\n",
    "        if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "            pos_word_list.append(word)\n",
    "        elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "            neg_word_list.append(word)\n",
    "        else:\n",
    "            neu_word_list.append(word)\n",
    "\n",
    "#remove duplicate words from each list            \n",
    "pos_words_unique = list(set(pos_word_list))\n",
    "neu_words_unique = list(set(neu_word_list))\n",
    "neg_words_unique = list(set(neg_word_list))\n",
    "\n",
    "#make all words lower case\n",
    "pos_words_unique = [item.lower() for item in pos_words_unique]\n",
    "neu_words_unique = [item.lower() for item in neu_words_unique]\n",
    "neg_word_unique = [item.lower() for item in neg_words_unique]\n",
    "\n",
    "#sort all words in lists alphabetically\n",
    "pos_words_unique.sort()\n",
    "neu_words_unique.sort()\n",
    "neg_words_unique.sort()\n",
    "\n",
    "\n",
    "# Create text file with sentiment classification of all corpus words\n",
    "# This enables us to see how the classifier in its current form classifies words\n",
    "# We can then amend the JSON file used above to improve performance of sentiment analysis by including new words/scores\n",
    "with open(\"outputs/word_sentiments_mandela.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"positive polarity words\\n\")\n",
    "    f.write(\"\\n\".join(pos_words_unique))\n",
    "    f.write(\"\\n\\n\")\n",
    "    f.write(\"neutral polarity words\\n\")\n",
    "    f.write(\"\\n\".join(neu_words_unique))\n",
    "    f.write(\"\\n\\n\")\n",
    "    f.write(\"negative polarity words\\n\")\n",
    "    f.write(\"\\n\".join(neg_words_unique))\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "#sentiment score for each sentence returns a dictionary of different scores\n",
    "#divide dictionary values into lists for each score\n",
    "#we can then use these lists to create any number of dataframes for different perspectives on the data\n",
    "neg = []\n",
    "neu = []\n",
    "pos = []\n",
    "compound = []\n",
    "    \n",
    "for score in scores:\n",
    "    for key, value in score.items():\n",
    "        if key == \"neg\":\n",
    "            neg.append(value)\n",
    "        if key == \"neu\":\n",
    "            neu.append(value)\n",
    "        if key == \"pos\":\n",
    "            pos.append(value)\n",
    "        if key == \"compound\":\n",
    "            compound.append(value)\n",
    "            \n",
    "#create dictionary then dataframe without neutral and add positive and negative together for strong emotions using lists above\n",
    "#this enables us to see the strong emotion scores for each sentence without compound or neutral scores\n",
    "strong_emotion_data = {}\n",
    "\n",
    "strong_emotion_data[\"file\"] = files\n",
    "strong_emotion_data[\"year\"] = years\n",
    "strong_emotion_data[\"speech_id\"] = speech_ids\n",
    "strong_emotion_data[\"sentence\"] = sentence_list        \n",
    "strong_emotion_data[\"negative\"] = neg\n",
    "strong_emotion_data[\"positive\"] = pos                       \n",
    "        \n",
    "#create pandas dataframe to make data exports/manipulation easier\n",
    "strong_df = pd.DataFrame(strong_emotion_data)\n",
    "\n",
    "#remove sentences that have been turned into full stops by data cleaning\n",
    "strong_df = strong_df[strong_df.sentence != \".\"]\n",
    "\n",
    "#set index as speech_id\n",
    "strong_df.set_index(\"speech_id\", inplace=True)\n",
    "\n",
    "#convert negative into positive and add columns together, create new sum column for results\n",
    "strong_df[\"negative\"] = strong_df[\"negative\"].abs()\n",
    "sum_column = strong_df[\"positive\"] + strong_df[\"negative\"]\n",
    "strong_df[\"sum positive/negative\"] = sum_column\n",
    "\n",
    "#save dataframe to csv with all results\n",
    "strong_df.to_csv(\"outputs/mandela_vader_strong_non_compound_scores_sentiment_all.csv\")\n",
    "\n",
    "#divide dataframe into low scoring and high scoring emotion, the parameters can be changed\n",
    "strong_high_score_df = strong_df.loc[(strong_df[\"sum positive/negative\"] >= 0.3)]\n",
    "low_high_score_df = strong_df.loc[(strong_df[\"sum positive/negative\"] <= 0.1)]\n",
    "\n",
    "#save the above dataframes to csv files\n",
    "strong_high_score_df.to_csv(\"outputs/mandela_vader_strong_non_compound_scores_sentiment_high.csv\")\n",
    "low_high_score_df.to_csv(\"outputs/mandela_vader_strong_non_compound_scores_sentiment_low.csv\")\n",
    "            \n",
    "#create main dataframe, which includes/uses neutral and compound scores.\n",
    "#compound score takes into account sentence syntax for creating a positive/negative score\n",
    "data = {}\n",
    "\n",
    "data[\"file\"] = files\n",
    "data[\"year\"] = years\n",
    "data[\"speech_id\"] = speech_ids\n",
    "data[\"sentence\"] = sentence_list        \n",
    "data[\"negative\"] = neg\n",
    "data[\"neutral\"] = neu\n",
    "data[\"positive\"] = pos\n",
    "data[\"compound score\"] = compound  \n",
    "\n",
    "#create pandas dataframe to make data exports/manipulation easier\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "#remove sentences that have been turned into full stops by data cleaning\n",
    "df = df[df.sentence != \".\"]   \n",
    "\n",
    "#create new dataframes with high and neutral sentiment windows, divide data accordingly\n",
    "#windows can be changed\n",
    "df_pos_neg = df.loc[(df['compound score'] <= -0.8) | (df['compound score'] >= 0.8)]\n",
    "df_neutral = df.loc[(df['compound score'] >= -0.2) & (df['compound score'] <= 0.2)]\n",
    "\n",
    "#save dataframes to csv, index to speech_id\n",
    "df_pos_neg.set_index(\"speech_id\", inplace=True)\n",
    "df_pos_neg.to_csv(\"outputs/mandela_vader_pos_neg_speech_sentiment_analysis_scores.csv\")\n",
    "\n",
    "df_neutral.set_index(\"speech_id\", inplace=True)\n",
    "df_neutral.to_csv(\"outputs/mandela_vader_neutral_speech_sentiment_analysis_scores.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
